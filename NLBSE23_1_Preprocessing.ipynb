{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install ekphrasis scikit-learn pandas numpy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MDf2CY9hjLOg",
        "outputId": "f62aefa3-6bba-4d94-b220-30286dc847ad"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: ekphrasis in /usr/local/lib/python3.8/dist-packages (0.5.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.8/dist-packages (1.0.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (1.3.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (1.21.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from ekphrasis) (4.64.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.8/dist-packages (from ekphrasis) (3.2.2)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.8/dist-packages (from ekphrasis) (2.2.0)\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.8/dist-packages (from ekphrasis) (6.1.1)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.8/dist-packages (from ekphrasis) (0.4.6)\n",
            "Requirement already satisfied: ujson in /usr/local/lib/python3.8/dist-packages (from ekphrasis) (5.7.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.8/dist-packages (from ekphrasis) (3.7)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn) (1.7.3)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn) (3.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.8/dist-packages (from scikit-learn) (1.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas) (2022.7.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n",
            "Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.8/dist-packages (from ftfy->ekphrasis) (0.2.6)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->ekphrasis) (1.4.4)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib->ekphrasis) (0.11.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->ekphrasis) (3.0.9)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.8/dist-packages (from nltk->ekphrasis) (2022.6.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from nltk->ekphrasis) (7.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "from ekphrasis.classes.preprocessor import TextPreProcessor\n",
        "from ekphrasis.classes.tokenizer import SocialTokenizer\n",
        "from ekphrasis.dicts.emoticons import emoticons\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import re\n",
        "import pandas as pd\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "import signal\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "u7dLtqCPjUJf"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "qiel7uini3Mt"
      },
      "outputs": [],
      "source": [
        "# download the training set if it does not exist\n",
        "train_file = 'nlbse23-issue-classification-train.csv'\n",
        "test_file = 'nlbse23-issue-classification-test.csv'\n",
        "\n",
        "if not os.path.isfile(train_file):\n",
        "  !curl \"https://tickettagger.blob.core.windows.net/datasets/{train_file}.tar.gz\" | tar -xz\n",
        "\n",
        "\n",
        "if not os.path.isfile(test_file):\n",
        "  !curl \"https://tickettagger.blob.core.windows.net/datasets/{test_file}.tar.gz\" | tar -xz"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "image_regex = re.compile('!\\[(.*)\\]\\(.*\\)')\n",
        "link_regex_1 = re.compile('\\[(.*)\\]\\(.*\\)')\n",
        "link_regex_2 = re.compile('\\[(.*)\\]: [^\\s]+')\n",
        "code_regex = re.compile('(:?`[^`]+`|```[^`]*```)')\n",
        "\n",
        "# Define label mapping\n",
        "label2int = {\n",
        "    \"bug\": 0,\n",
        "    \"documentation\" : 1,\n",
        "    \"feature\" : 2,\n",
        "    \"question\" : 3,\n",
        "}\n",
        "\n",
        "def preprocess_raw(output_filepath=''):\n",
        "    \"\"\" preprocesses NLBSE23 raw data (data/raw) and saves it (data/processed)\n",
        "    \"\"\"\n",
        "    logger = logging.getLogger(__name__)\n",
        "    logger.info('preprocessing data set from raw data')\n",
        "    \n",
        "    train_set = pd.read_csv(\"./nlbse23-issue-classification-train.csv\")\n",
        "    test_set = pd.read_csv(\"./nlbse23-issue-classification-test.csv\")\n",
        "\n",
        "    train_set.drop_duplicates(subset=['id'], inplace=True)\n",
        "    test_set.drop_duplicates(subset=['id'], inplace=True)\n",
        "\n",
        "    lenc = lambda x: label2int[x]\n",
        "\n",
        "    train_set = preprocess_rows(train_set, lenc)\n",
        "    test_set = preprocess_rows(test_set, lenc)\n",
        "\n",
        "    train_set.replace({pd.NA: np.nan, '': np.nan}, inplace=True)\n",
        "    test_set.replace({pd.NA: np.nan, '': np.nan}, inplace=True)\n",
        "\n",
        "    train_set.dropna(subset=['text'], inplace=True)\n",
        "    test_set.dropna(subset=['text'], inplace=True)\n",
        "\n",
        "    train_set.to_csv(os.path.join(output_filepath, 'train_set.csv'), index=False)\n",
        "    test_set.to_csv(os.path.join(output_filepath, 'test_set.csv'), index=False)\n",
        "\n",
        "\n",
        "def get_ekphrasis_preprocessor():\n",
        "    return TextPreProcessor(\n",
        "    # terms that will be normalized\n",
        "    normalize=['url', 'email', 'percent', 'money', 'phone', 'user',\n",
        "        'time', 'url', 'date', 'number'],\n",
        "    # terms that will be annotated\n",
        "    annotate={\"hashtag\", \"allcaps\", \"elongated\", \"repeated\",\n",
        "        'emphasis', 'censored'},\n",
        "    fix_html=True,  # fix HTML tokens\n",
        "    \n",
        "    # corpus from which the word statistics are going to be used \n",
        "    # for word segmentation \n",
        "    segmenter=\"twitter\", \n",
        "    \n",
        "    # corpus from which the word statistics are going to be used \n",
        "    # for spell correction\n",
        "    corrector=\"twitter\", \n",
        "    \n",
        "    unpack_hashtags=True,  # perform word segmentation on hashtags\n",
        "    unpack_contractions=True,  # Unpack contractions (can't -> can not)\n",
        "    spell_correct_elong=False,  # spell correction for elongated words\n",
        "    \n",
        "    # select a tokenizer. You can use SocialTokenizer, or pass your own\n",
        "    # the tokenizer, should take as input a string and return a list of tokens\n",
        "    tokenizer=SocialTokenizer(lowercase=True).tokenize,\n",
        "    \n",
        "    # list of dictionaries, for replacing tokens extracted from the text,\n",
        "    # with other expressions. You can pass more than one dictionaries.\n",
        "    dicts=[emoticons]\n",
        "    )\n",
        "\n",
        "def preprocess_rows(df, label_encoder):\n",
        "    logger = logging.getLogger(__name__)\n",
        "    logger.info('started preprocessing rows')\n",
        "    df = df.fillna({\n",
        "                        'title': '',\n",
        "                        'body': ''                  \n",
        "                   })\n",
        "    df['text'] = df['title'] + df['body']\n",
        "    df['label'] = [label_encoder(x) for x in df['labels']]\n",
        "    df = df.filter(['id', 'text', 'label'])\n",
        "    text_processor = get_ekphrasis_preprocessor()\n",
        "    df['text'] = [clean_text(text, text_processor) for text in tqdm(df['text'])]\n",
        "    return df\n",
        "\n",
        "class TimeoutException(Exception):   # Custom exception class\n",
        "    pass\n",
        "\n",
        "def timeout_handler(signum, frame):   # Custom signal handler\n",
        "    raise TimeoutException\n",
        "\n",
        "def clean_text(text, text_processor):\n",
        "    \n",
        "    #bar.set_description('regex')\n",
        "    cleaned = re.sub(image_regex, r'\\1 <img>', text)\n",
        "    cleaned = re.sub(link_regex_1, r'\\1 <url>', cleaned)\n",
        "    cleaned = re.sub(link_regex_2, r'\\1 <url>', cleaned)\n",
        "    cleaned = re.sub(code_regex, '<code>', cleaned)\n",
        "    #bar.set_description('ekph')\n",
        "    signal.signal(signal.SIGALRM, timeout_handler)\n",
        "    \n",
        "    signal.alarm(5)\n",
        "    \n",
        "    try:\n",
        "        cleaned = \" \".join(text_processor.pre_process_doc(cleaned))\n",
        "    except (RecursionError, TimeoutException):\n",
        "        cleaned = pd.NA\n",
        "    else:\n",
        "        signal.alarm(0)\n",
        "    #bar.set_description('end')\n",
        "    return cleaned"
      ],
      "metadata": {
        "id": "n-_Va2RljRkX"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "log_fmt = '%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
        "logging.basicConfig(level=logging.INFO, format=log_fmt)\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "logger.info('making final data set from raw data')\n",
        "\n",
        "preprocess_raw('.')\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hlRPxl87kCQq",
        "outputId": "9dc16723-b655-40ae-9c5c-0e972f592180"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/ekphrasis/classes/tokenizer.py:225: FutureWarning: Possible nested set at position 2190\n",
            "  self.tok = re.compile(r\"({})\".format(\"|\".join(pipeline)))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading twitter - 1grams ...\n",
            "Reading twitter - 2grams ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/ekphrasis/classes/exmanager.py:14: FutureWarning: Possible nested set at position 42\n",
            "  regexes = {k.lower(): re.compile(self.expressions[k]) for k, v in\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading twitter - 1grams ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 99726/99726 [04:03<00:00, 409.25it/s] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading twitter - 1grams ...\n",
            "Reading twitter - 2grams ...\n",
            "Reading twitter - 1grams ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 99718/99718 [03:48<00:00, 437.29it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!curl -LJO \"https://zenodo.org/record/7628150/files/test_set_r.csv\"\n",
        "!curl -LJO \"https://zenodo.org/record/7628150/files/train_set_r.csv\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tjPZN06LrX7J",
        "outputId": "01030bce-888b-433d-9c1a-3786fffd158b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0Warning: Refusing to overwrite test_set_r.csv: File exists\n",
            "  0  3801    0     0    0     0      0      0 --:--:--  0:00:01 --:--:--     0\n",
            "curl: (23) Failed writing header\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0Warning: Refusing to overwrite train_set_r.csv: File exists\n",
            "  0  3833    0     0    0     0      0      0 --:--:--  0:00:01 --:--:--     0\n",
            "curl: (23) Failed writing header\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "\n",
        "# Preprocess dataset\n",
        "\n",
        "train_set = pd.read_csv(\"./train_set.csv\")\n",
        "test_set = pd.read_csv(\"./test_set.csv\")\n",
        "\n",
        "train_set.drop_duplicates(subset=['id'], inplace=True)\n",
        "test_set.drop_duplicates(subset=['id'], inplace=True)\n",
        "\n",
        "# -- Open df\n",
        "train_sample = pd.read_csv('./train_set_r.csv')\n",
        "test_sample = pd.read_csv('./test_set_r.csv')\n",
        "\n",
        "train_df = pd.merge(train_sample, train_set, on=['id'], how='inner')\n",
        "test_df = pd.merge(test_sample, train_set, on=['id'], how='inner')\n",
        "\n",
        "def filter_df(df):\n",
        "    df = df[~df[\"new_label\"].isin([\"unknown\", \"discard\"])]\n",
        "    df[\"label\"] = df[\"new_label\"].map(label2int).tolist()\n",
        "    df = df.drop([\"new_label\"], axis=1)\n",
        "    return df\n",
        "\n",
        "train_df = filter_df(train_df)\n",
        "test_df = filter_df(test_df)\n",
        "\n",
        "# -- Plot distribution\n",
        "def plot_dist(df):\n",
        "    print(df[\"label\"].value_counts())\n",
        "    \n",
        "plot_dist(train_df)\n",
        "plot_dist(test_df)\n",
        "\n",
        "train_df.to_csv(\"train_set_hand.csv\", index=False)\n",
        "test_df.to_csv(\"test_set_hand.csv\", index=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OjTUYJ1Cqbhe",
        "outputId": "4a3b15f1-9347-4f2d-90b8-a95a9a378a2f"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2    5\n",
            "1    4\n",
            "0    2\n",
            "3    2\n",
            "Name: label, dtype: int64\n",
            "Series([], Name: label, dtype: int64)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-10-8179d8c5b694>:25: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df[\"label\"] = labels\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bpgq3b1PGsvx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}