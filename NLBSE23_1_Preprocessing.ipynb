{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MDf2CY9hjLOg",
        "outputId": "f62aefa3-6bba-4d94-b220-30286dc847ad"
      },
      "outputs": [],
      "source": [
        "!pip install ekphrasis scikit-learn pandas numpy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "u7dLtqCPjUJf"
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "from ekphrasis.classes.preprocessor import TextPreProcessor\n",
        "from ekphrasis.classes.tokenizer import SocialTokenizer\n",
        "from ekphrasis.dicts.emoticons import emoticons\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import re\n",
        "import pandas as pd\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "import signal\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "qiel7uini3Mt"
      },
      "outputs": [],
      "source": [
        "# download the training set if it does not exist\n",
        "train_file = 'nlbse23-issue-classification-train.csv'\n",
        "test_file = 'nlbse23-issue-classification-test.csv'\n",
        "\n",
        "if not os.path.isfile(train_file):\n",
        "  !curl \"https://tickettagger.blob.core.windows.net/datasets/{train_file}.tar.gz\" | tar -xz\n",
        "\n",
        "\n",
        "if not os.path.isfile(test_file):\n",
        "  !curl \"https://tickettagger.blob.core.windows.net/datasets/{test_file}.tar.gz\" | tar -xz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "n-_Va2RljRkX"
      },
      "outputs": [],
      "source": [
        "image_regex = re.compile('!\\[(.*)\\]\\(.*\\)')\n",
        "link_regex_1 = re.compile('\\[(.*)\\]\\(.*\\)')\n",
        "link_regex_2 = re.compile('\\[(.*)\\]: [^\\s]+')\n",
        "code_regex = re.compile('(:?`[^`]+`|```[^`]*```)')\n",
        "\n",
        "# Define label mapping\n",
        "label2int = {\n",
        "    \"bug\": 0,\n",
        "    \"documentation\" : 1,\n",
        "    \"feature\" : 2,\n",
        "    \"question\" : 3,\n",
        "}\n",
        "\n",
        "def preprocess_raw(output_filepath=''):\n",
        "    \"\"\" preprocesses NLBSE23 raw data (data/raw) and saves it (data/processed)\n",
        "    \"\"\"\n",
        "    logger = logging.getLogger(__name__)\n",
        "    logger.info('preprocessing data set from raw data')\n",
        "    \n",
        "    train_set = pd.read_csv(\"./nlbse23-issue-classification-train.csv\")\n",
        "    test_set = pd.read_csv(\"./nlbse23-issue-classification-test.csv\")\n",
        "\n",
        "    train_set.drop_duplicates(subset=['id'], inplace=True)\n",
        "    test_set.drop_duplicates(subset=['id'], inplace=True)\n",
        "\n",
        "    lenc = lambda x: label2int[x]\n",
        "\n",
        "    train_set = preprocess_rows(train_set, lenc)\n",
        "    test_set = preprocess_rows(test_set, lenc)\n",
        "\n",
        "    train_set.replace({pd.NA: np.nan, '': np.nan}, inplace=True)\n",
        "    test_set.replace({pd.NA: np.nan, '': np.nan}, inplace=True)\n",
        "\n",
        "    train_set.dropna(subset=['text'], inplace=True)\n",
        "    test_set.dropna(subset=['text'], inplace=True)\n",
        "\n",
        "    train_set.to_csv(os.path.join(output_filepath, 'train_set.csv'), index=False)\n",
        "    test_set.to_csv(os.path.join(output_filepath, 'test_set.csv'), index=False)\n",
        "\n",
        "\n",
        "def get_ekphrasis_preprocessor():\n",
        "    return TextPreProcessor(\n",
        "    # terms that will be normalized\n",
        "    normalize=['url', 'email', 'percent', 'money', 'phone', 'user',\n",
        "        'time', 'url', 'date', 'number'],\n",
        "    # terms that will be annotated\n",
        "    annotate={\"hashtag\", \"allcaps\", \"elongated\", \"repeated\",\n",
        "        'emphasis', 'censored'},\n",
        "    fix_html=True,  # fix HTML tokens\n",
        "    \n",
        "    # corpus from which the word statistics are going to be used \n",
        "    # for word segmentation \n",
        "    segmenter=\"twitter\", \n",
        "    \n",
        "    # corpus from which the word statistics are going to be used \n",
        "    # for spell correction\n",
        "    corrector=\"twitter\", \n",
        "    \n",
        "    unpack_hashtags=True,  # perform word segmentation on hashtags\n",
        "    unpack_contractions=True,  # Unpack contractions (can't -> can not)\n",
        "    spell_correct_elong=False,  # spell correction for elongated words\n",
        "    \n",
        "    # select a tokenizer. You can use SocialTokenizer, or pass your own\n",
        "    # the tokenizer, should take as input a string and return a list of tokens\n",
        "    tokenizer=SocialTokenizer(lowercase=True).tokenize,\n",
        "    \n",
        "    # list of dictionaries, for replacing tokens extracted from the text,\n",
        "    # with other expressions. You can pass more than one dictionaries.\n",
        "    dicts=[emoticons]\n",
        "    )\n",
        "\n",
        "def preprocess_rows(df, label_encoder):\n",
        "    logger = logging.getLogger(__name__)\n",
        "    logger.info('started preprocessing rows')\n",
        "    df = df.fillna({\n",
        "                        'title': '',\n",
        "                        'body': ''                  \n",
        "                   })\n",
        "    df['text'] = df['title'] + df['body']\n",
        "    df['label'] = [label_encoder(x) for x in df['labels']]\n",
        "    df = df.filter(['id', 'text', 'label'])\n",
        "    text_processor = get_ekphrasis_preprocessor()\n",
        "    df['text'] = [clean_text(text, text_processor) for text in tqdm(df['text'])]\n",
        "    return df\n",
        "\n",
        "class TimeoutException(Exception):   # Custom exception class\n",
        "    pass\n",
        "\n",
        "def timeout_handler(signum, frame):   # Custom signal handler\n",
        "    raise TimeoutException\n",
        "\n",
        "def clean_text(text, text_processor):\n",
        "    \n",
        "    #bar.set_description('regex')\n",
        "    cleaned = re.sub(image_regex, r'\\1 <img>', text)\n",
        "    cleaned = re.sub(link_regex_1, r'\\1 <url>', cleaned)\n",
        "    cleaned = re.sub(link_regex_2, r'\\1 <url>', cleaned)\n",
        "    cleaned = re.sub(code_regex, '<code>', cleaned)\n",
        "    #bar.set_description('ekph')\n",
        "    signal.signal(signal.SIGALRM, timeout_handler)\n",
        "    \n",
        "    signal.alarm(5)\n",
        "    \n",
        "    try:\n",
        "        cleaned = \" \".join(text_processor.pre_process_doc(cleaned))\n",
        "    except (RecursionError, TimeoutException):\n",
        "        cleaned = pd.NA\n",
        "    else:\n",
        "        signal.alarm(0)\n",
        "    #bar.set_description('end')\n",
        "    return cleaned"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hlRPxl87kCQq",
        "outputId": "9dc16723-b655-40ae-9c5c-0e972f592180"
      },
      "outputs": [],
      "source": [
        "log_fmt = '%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
        "logging.basicConfig(level=logging.INFO, format=log_fmt)\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "logger.info('making final data set from raw data')\n",
        "\n",
        "preprocess_raw('.')\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tjPZN06LrX7J",
        "outputId": "01030bce-888b-433d-9c1a-3786fffd158b"
      },
      "outputs": [],
      "source": [
        "!curl -LJO \"https://zenodo.org/record/7628150/files/test_set_r.csv\"\n",
        "!curl -LJO \"https://zenodo.org/record/7628150/files/train_set_r.csv\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OjTUYJ1Cqbhe",
        "outputId": "4a3b15f1-9347-4f2d-90b8-a95a9a378a2f"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "\n",
        "# Preprocess dataset\n",
        "\n",
        "train_set = pd.read_csv(\"./train_set.csv\")\n",
        "test_set = pd.read_csv(\"./test_set.csv\")\n",
        "\n",
        "train_set.drop_duplicates(subset=['id'], inplace=True)\n",
        "test_set.drop_duplicates(subset=['id'], inplace=True)\n",
        "\n",
        "# -- Open df\n",
        "train_sample = pd.read_csv('./train_set_r.csv')\n",
        "test_sample = pd.read_csv('./test_set_r.csv')\n",
        "\n",
        "train_df = pd.merge(train_sample, train_set, on=['id'], how='inner')\n",
        "test_df = pd.merge(test_sample, train_set, on=['id'], how='inner')\n",
        "\n",
        "def filter_df(df):\n",
        "    df = df[~df[\"new_label\"].isin([\"unknown\", \"discard\"])]\n",
        "    df[\"label\"] = df[\"new_label\"].map(label2int).tolist()\n",
        "    df = df.drop([\"new_label\"], axis=1)\n",
        "    return df\n",
        "\n",
        "train_df = filter_df(train_df)\n",
        "test_df = filter_df(test_df)\n",
        "\n",
        "# -- Plot distribution\n",
        "def plot_dist(df):\n",
        "    print(df[\"label\"].value_counts())\n",
        "    \n",
        "plot_dist(train_df)\n",
        "plot_dist(test_df)\n",
        "\n",
        "train_df.to_csv(\"train_set_hand.csv\", index=False)\n",
        "test_df.to_csv(\"test_set_hand.csv\", index=False)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
